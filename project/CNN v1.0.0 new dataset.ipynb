{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a736eb632441>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import relevant packages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Import relevant packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "DISABLE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = [16, 32, 64, 128]\n",
    "lr = [0.001, 0.0001, 0.00001]\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Declare important file paths\n",
    "notebook_path = os.path.abspath(\"Custom_CNN.ipynb\")\n",
    "data_path = os.path.dirname(notebook_path) + '/project/data/columbia-prcg-datasets/'\n",
    "model_path = os.path.dirname(notebook_path) + '/project/model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select accelerator device\n",
    "def get_default_device():\n",
    "    \"\"\"Returns device, is_cuda (bool).\"\"\"\n",
    "    if not DISABLE_CUDA and torch.cuda.is_available():\n",
    "        print(\"Running on CUDA!\")\n",
    "        return torch.device('cuda'), True\n",
    "    else:\n",
    "        print(\"Running on CPU!\")\n",
    "        return torch.device('cpu'), False\n",
    "device, using_cuda = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the hyperparameters, this function runs a full experiment and returns lists of loss\n",
    "def run_experiment(input_dim, lr, crop=False, train_test_ratio=0.8):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(input_dim) if crop else transforms.Resize(input_dim),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    full_set = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    train_size = int(train_test_ratio * len(full_set))\n",
    "    val_size = (len(full_set) - train_size) / 2\n",
    "    test_size = len(full_set) - train_size - val_size\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(full_set, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, num_workers=1)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, num_workers=1)\n",
    "\n",
    "    # Declare our model architecture\n",
    "    class ConvNet(nn.Module):  # Convolutional Neural Network\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  \n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  \n",
    "            # self.drop_out = nn.Dropout()\n",
    "            self.fc1 = nn.Linear(input_dim * input_dim * 0.25 * 0.25 * 64, 1)\n",
    "            # self.fc1 = nn.Linear(128 * 128 * 64, 1000)\n",
    "            # self.fc2 = nn.Linear(1000, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.layer1(x)\n",
    "            out = self.layer2(out)\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            # out = self.drop_out(out)\n",
    "            out = self.fc1(out)\n",
    "            # out = self.fc2(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    loss_list, train_accuracy_list, val_accuracy_list = train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs=30)\n",
    "    return loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(input_dim=16, lr=0.001, crop=False, train_test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare our model architecture\n",
    "class ConvNet(nn.Module):  # Convolutional Neural Network\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),  # (512, 512, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  # (256, 256, 32)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  # (256, 256, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  #  (128, 128, 64)\n",
    "        # self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(128 * 128 * 64, 1)\n",
    "        # self.fc1 = nn.Linear(128 * 128 * 64, 1000)\n",
    "        # self.fc2 = nn.Linear(1000, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        # out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs):\n",
    "    loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "\n",
    "    t = torch.Tensor([0.5]).to(device)  # 0.5 acts as threshold\n",
    "    torch.backends.cudnn.benchmark = True  # make training faster on Cuda\n",
    "    # start_time = time.time()\n",
    "\n",
    "    model.train()  # switch to train mode       \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        running_loss = 0.0\n",
    "        train_correct = train_total = 0 \n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs = data[0].to(device, non_blocking=True)\n",
    "            labels = data[1].to(device, non_blocking=True)\n",
    "        \n",
    "            probs = model(inputs)\n",
    "\n",
    "            outputs = (probs > t).float() * 1  # obtain train accuracies\n",
    "            train_total += len(outputs)\n",
    "            train_correct += (outputs == labels.float()).float().sum()\n",
    "\n",
    "            loss = loss_fn(probs, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # if False and (i + 1) % 10 == 0:\n",
    "            #     print ('# Images: {:} | Loss: {:.6f} | Time: {:.6f}'.format(i + 1, running_loss / (i + 1), time.time() - start_time))\n",
    "        train_accuracy = train_correct / train_total\n",
    "            \n",
    "        # Test current version of model to obtain accuracy    \n",
    "        val_correct = val_total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs = data[0].to(device, non_blocking=True)\n",
    "                labels = data[1].to(device, non_blocking=True)\n",
    "                probs = model(inputs)\n",
    "                outputs = (probs > t).float() * 1\n",
    "                val_total += len(outputs)\n",
    "                val_correct += (outputs == labels.float()).float().sum()\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        loss_list.append(running_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        # print ('Epoch: {:} | Time (m): {:.6f} | Loss: {:.6f} | Test Accuracy: {:}'.format(epoch, (time.time() - start_time)/60, running_loss, test_accuracy))\n",
    "\n",
    "    return loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Custom_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}