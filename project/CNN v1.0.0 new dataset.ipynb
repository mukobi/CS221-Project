{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "DISABLE_CUDA = False\n",
    "\n",
    "NOTEBOOK_NAME = 'CNN v1.0.0 new dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = 128\n",
    "LR = 0.0001\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Declare important file paths\n",
    "project_path = os.path.abspath('')\n",
    "data_path = project_path + '/data/ldrd-and-raise-datasets/image-folder'\n",
    "model_path = project_path + '/models/' + NOTEBOOK_NAME + '-model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Running on CPU!\n"
    }
   ],
   "source": [
    "# Select accelerator device\n",
    "def get_default_device():\n",
    "    \"\"\"Returns device, is_cuda (bool).\"\"\"\n",
    "    if not DISABLE_CUDA and torch.cuda.is_available():\n",
    "        print(\"Running on CUDA!\")\n",
    "        return torch.device('cuda'), True\n",
    "    else:\n",
    "        print(\"Running on CPU!\")\n",
    "        return torch.device('cpu'), False\n",
    "device, using_cuda = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_data(input_dim):\n",
    "    # Transform the data\n",
    "    transform = transforms.Compose([\n",
    "                        transforms.Resize((input_dim, input_dim)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Create training/testing dataloaders\n",
    "    full_set = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    train_size = int(train_test_ratio * len(full_set))\n",
    "    val_size = int((len(full_set) - train_size) / 2)\n",
    "    test_size = len(full_set) - train_size - val_size\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(full_set, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "    # train_data_in_memory = load_data_into_memory(train_loader)\n",
    "    # val_data_in_memory = load_data_into_memory(val_loader)\n",
    "    # return train_data_in_memory, val_data_in_memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data into memory to elimate read bottleneck\n",
    "def load_data_into_memory(data_loader):\n",
    "    output = []\n",
    "    for data in data_loader:\n",
    "        inputs = data[0].to(device, non_blocking=True)\n",
    "        labels = data[1].to(device, non_blocking=True)\n",
    "        output.append((inputs, labels))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare our model architecture\n",
    "def declare_model(input_dim):\n",
    "    class ConvNet(nn.Module):  # Convolutional Neural Network\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),  # (512, 512, 32) (256, 256, 32)\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  # (256, 256, 32)\n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=5, stride=2, padding=2),  # (256, 256, 64)\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  #  (128, 128, 64)\n",
    "            self.layer3 = nn.Sequential(\n",
    "                nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),  # (512, 512, 64)\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  #  (64, 64, 64)\n",
    "    #         self.drop_out = nn.Dropout(0.1)\n",
    "            self.fc1 = nn.Linear(int(input_dim/8) * int(input_dim/8) * 8, 32)\n",
    "            self.fc2 = nn.Linear(32, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            # print (x.shape)\n",
    "            out = self.layer1(x)\n",
    "            # print (out.shape)\n",
    "            out = self.layer2(out)\n",
    "            # print (out.shape)\n",
    "            out = self.layer3(out)\n",
    "            # print (out.shape)\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            # print (out.shape)\n",
    "    #         out = self.drop_out(out)\n",
    "            out = self.fc1(out)\n",
    "            # print (out.shape)\n",
    "            out = self.fc2(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs):\n",
    "    loss_list = []\n",
    "    time_list = []\n",
    "    train_accuracy_list = []\n",
    "    val_accuracy_list = []\n",
    "    t = torch.Tensor([0.5]).to(device)  # 0.5 acts as threshold\n",
    "    # highest_acc = 0.0\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True  # make training faster on Cuda\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()  # switch to train mode\n",
    "        \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        running_loss = 0.0\n",
    "        train_correct = train_total = 0 \n",
    "        for i, (inputs, labels) in enumerate(load_data_into_memory(train_loader)):\n",
    "            labels = labels.view(-1,1)\n",
    "\n",
    "            probs = model(inputs)\n",
    "\n",
    "            outputs = (probs > t).float() * 1  # obtain train accuracies\n",
    "            train_total += len(outputs)\n",
    "            train_correct += (outputs == labels.float()).float().sum() / len(outputs)  # normalize batch size\n",
    "\n",
    "            loss = loss_fn(probs, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # if (i + 1) % 10 == 0:\n",
    "            #     print ('# Images: {:} | Time (m): {:.3f} | Loss: {:.6f} '.format(i + 1, (time.time() - start_time)/60, running_loss / (i + 1)))\n",
    "        train_accuracy = train_correct / train_total\n",
    "            \n",
    "        # Test current version of model to obtain accuracy    \n",
    "        val_correct = val_total = 0 \n",
    "        with torch.no_grad():\n",
    "            for (inputs, labels) in load_data_into_memory(val_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                labels = labels.view(-1,1)\n",
    "\n",
    "                probs = model(inputs)\n",
    "                outputs = (probs > t).float() * 1\n",
    "                val_total += len(outputs)\n",
    "                val_correct += (outputs == labels.float()).float().sum() / len(outputs)  # normalize batch size\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # if val_accuracy > highest_acc:  # save highest accuracy model\n",
    "        #     highest_acc = val_accuracy\n",
    "        #     torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        elapsed_time = (time.time() - start_time)/60\n",
    "        time_list.append(elapsed_time)\n",
    "        loss_list.append(running_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        print ('Epoch: {:} | Time (m): {:.6f} | Loss: {:.6f} | Train Accuracy: {:.8%} | Validation Accuracy: {:.8%}'.format(\n",
    "            epoch, elapsed_time, running_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "    return time_list, loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_experiment_results_to_file(filename, results_dict):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(results_dict.keys())\n",
    "        num_rows = len(list(results_dict.values())[0])\n",
    "        for i in range(num_rows):\n",
    "            row = []\n",
    "            for key in results_dict.keys():\n",
    "                row.append(float(results_dict[key][i]))\n",
    "            writer.writerow(row)\n",
    "        print('Wrote {} rows to file.'.format(num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(input_dim, lr, num_epochs):\n",
    "    train_loader, val_loader = obtain_data(input_dim)\n",
    "\n",
    "    model = declare_model(input_dim)\n",
    "    # Define the loss function and optimizer\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    time_list, loss_list, train_accuracy_list, val_accuracy_list = train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs=num_epochs)\n",
    "\n",
    "    results_filename = \"experiments/chkpt 1 ~ dim={}, lr={}, epochs={}, cuda={}.csv\".format(input_dim, lr, num_epochs, using_cuda)\n",
    "    print(results_filename)\n",
    "    results_dict = {\"time (m)\": time_list, \"loss\": loss_list, \"train accuracy\": train_accuracy_list, \"val accuracy\": val_accuracy_list}\n",
    "    write_experiment_results_to_file(results_filename, results_dict)\n",
    "\n",
    "    return time_list, loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment with \n",
    "for input_dim in [128]:\n",
    "    for lr in [0.0001]:\n",
    "        num_epochs = 20\n",
    "        time_list, loss_list, train_accuracy_list, val_accuracy_list = run_experiment(input_dim, lr, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Custom_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}