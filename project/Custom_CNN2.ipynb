{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flags\n",
    "DISABLE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = [16, 32, 64, 128]\n",
    "lr = [0.001, 0.0001, 0.00001]\n",
    "train_test_ratio = 0.8\n",
    "\n",
    "# Declare important file paths\n",
    "notebook_path = os.path.abspath(\"Custom_CNN.ipynb\")\n",
    "data_path = os.path.dirname(notebook_path) + '/project/data/columbia-prcg-datasets/'\n",
    "model_path = os.path.dirname(notebook_path) + '/project/model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Running on CPU!\n"
    }
   ],
   "source": [
    "# Select accelerator device\n",
    "def get_default_device():\n",
    "    \"\"\"Returns device, is_cuda (bool).\"\"\"\n",
    "    if not DISABLE_CUDA and torch.cuda.is_available():\n",
    "        print(\"Running on CUDA!\")\n",
    "        return torch.device('cuda'), True\n",
    "    else:\n",
    "        print(\"Running on CPU!\")\n",
    "        return torch.device('cpu'), False\n",
    "device, using_cuda = get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the hyperparameters, this function runs a full experiment and returns lists of loss\n",
    "def run_experiment(input_dim, lr, crop=False, train_test_ratio=0.8):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.RandomCrop(input_dim) if crop else transforms.Resize(input_dim),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    full_set = datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    train_size = int(train_test_ratio * len(full_set))\n",
    "    val_size = (len(full_set) - train_size) / 2\n",
    "    test_size = len(full_set) - train_size - val_size\n",
    "    train_set, val_set, test_set = torch.utils.data.random_split(full_set, [train_size, val_size, test_size])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, num_workers=1)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, num_workers=1)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, shuffle=False, num_workers=1)\n",
    "\n",
    "    # Declare our model architecture\n",
    "    class ConvNet(nn.Module):  # Convolutional Neural Network\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.layer1 = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  \n",
    "            self.layer2 = nn.Sequential(\n",
    "                nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  \n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2))  \n",
    "            # self.drop_out = nn.Dropout()\n",
    "            self.fc1 = nn.Linear(input_dim * input_dim * 0.25 * 0.25 * 64, 1)\n",
    "            # self.fc1 = nn.Linear(128 * 128 * 64, 1000)\n",
    "            # self.fc2 = nn.Linear(1000, 1)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def forward(self, x):\n",
    "            out = self.layer1(x)\n",
    "            out = self.layer2(out)\n",
    "            out = out.reshape(out.size(0), -1)\n",
    "            # out = self.drop_out(out)\n",
    "            out = self.fc1(out)\n",
    "            # out = self.fc2(out)\n",
    "            out = self.sigmoid(out)\n",
    "            return out\n",
    "    model = ConvNet()\n",
    "    model.to(device)\n",
    "\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    loss_list, train_accuracy_list, val_accuracy_list = train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs=30)\n",
    "    return loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randperm(): argument 'n' (position 1) must be int, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-fdbb95d30ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-21ee7069d503>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(input_dim, lr, crop, train_test_ratio)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mval_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36mrandom_split\u001b[0;34m(dataset, lengths)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sum of input lengths does not equal the length of the input dataset!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandperm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mSubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: randperm(): argument 'n' (position 1) must be int, not float"
     ]
    }
   ],
   "source": [
    "run_experiment(input_dim=16, lr=0.001, crop=False, train_test_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=1048576, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare our model architecture\n",
    "class ConvNet(nn.Module):  # Convolutional Neural Network\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),  # (512, 512, 32)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  # (256, 256, 32)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),  # (256, 256, 64)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))  #  (128, 128, 64)\n",
    "        # self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(128 * 128 * 64, 1)\n",
    "        # self.fc1 = nn.Linear(128 * 128 * 64, 1000)\n",
    "        # self.fc2 = nn.Linear(1000, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        # out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        # out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, optimizer, train_loader, val_loader, num_epochs):\n",
    "    loss_list = []\n",
    "    train_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "\n",
    "    t = torch.Tensor([0.5]).to(device)  # 0.5 acts as threshold\n",
    "    torch.backends.cudnn.benchmark = True  # make training faster on Cuda\n",
    "    # start_time = time.time()\n",
    "\n",
    "    model.train()  # switch to train mode       \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        running_loss = 0.0\n",
    "        train_correct = train_total = 0 \n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs = data[0].to(device, non_blocking=True)\n",
    "            labels = data[1].to(device, non_blocking=True)\n",
    "        \n",
    "            probs = model(inputs)\n",
    "\n",
    "            outputs = (probs > t).float() * 1  # obtain train accuracies\n",
    "            train_total += len(outputs)\n",
    "            train_correct += (outputs == labels.float()).float().sum()\n",
    "\n",
    "            loss = loss_fn(probs, labels.float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # if False and (i + 1) % 10 == 0:\n",
    "            #     print ('# Images: {:} | Loss: {:.6f} | Time: {:.6f}'.format(i + 1, running_loss / (i + 1), time.time() - start_time))\n",
    "        train_accuracy = train_correct / train_total\n",
    "            \n",
    "        # Test current version of model to obtain accuracy    \n",
    "        val_correct = val_total = 0 \n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs = data[0].to(device, non_blocking=True)\n",
    "                labels = data[1].to(device, non_blocking=True)\n",
    "                probs = model(inputs)\n",
    "                outputs = (probs > t).float() * 1\n",
    "                val_total += len(outputs)\n",
    "                val_correct += (outputs == labels.float()).float().sum()\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        loss_list.append(running_loss)\n",
    "        train_accuracy_list.append(train_accuracy)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        # print ('Epoch: {:} | Time (m): {:.6f} | Loss: {:.6f} | Test Accuracy: {:}'.format(epoch, (time.time() - start_time)/60, running_loss, test_accuracy))\n",
    "\n",
    "    return loss_list, train_accuracy_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Custom_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}